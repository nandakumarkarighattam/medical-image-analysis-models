{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from glob import glob \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from skimage.io import imread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##Import libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "# Base model with MobileNetV2\n",
    "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.VGG16(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False, \n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainable classification head\n",
    "maxpool_layer = keras.layers.GlobalMaxPooling2D()\n",
    "prediction_layer = keras.layers.Dense(1, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate = 0.0001\n",
    "\n",
    "classifier = keras.Sequential([\n",
    "    base_model,\n",
    "    maxpool_layer,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data importing and transforming and scaling\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling test data\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3513 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set = train_datagen.flow_from_directory(r\"C:\\Users\\Nanda Kumar\\Downloads\\chest-xray-images-for-classification-pneumonia/pneumonia/train\",\n",
    "                                               target_size=(64, 64),\n",
    "                                               batch_size=6,\n",
    "                                               class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1171 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(r\"C:\\Users\\Nanda Kumar\\Downloads\\chest-xray-images-for-classification-pneumonia/pneumonia/test\",\n",
    "                                               target_size=(64, 64),\n",
    "                                               batch_size=6,\n",
    "                                               class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NORMAL': 0, 'PNEUMONIA': 1}\n"
     ]
    }
   ],
   "source": [
    "#which is normal which is abnormal?\n",
    "label_map = (train_set.class_indices)\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "98/98 [==============================] - 99s 1s/step - loss: 0.5171 - accuracy: 0.7092 - val_loss: 0.4577 - val_accuracy: 0.8070\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 99s 1s/step - loss: 0.4483 - accuracy: 0.8044 - val_loss: 0.4026 - val_accuracy: 0.8991\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 100s 1s/step - loss: 0.4049 - accuracy: 0.8282 - val_loss: 0.3573 - val_accuracy: 0.8991\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 100s 1s/step - loss: 0.3561 - accuracy: 0.8469 - val_loss: 0.3248 - val_accuracy: 0.8991\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 98s 995ms/step - loss: 0.3528 - accuracy: 0.8410 - val_loss: 0.3044 - val_accuracy: 0.9035\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 99s 1s/step - loss: 0.3471 - accuracy: 0.8452 - val_loss: 0.2964 - val_accuracy: 0.9254\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 99s 1s/step - loss: 0.3169 - accuracy: 0.8656 - val_loss: 0.2812 - val_accuracy: 0.9254\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 97s 985ms/step - loss: 0.3062 - accuracy: 0.8769 - val_loss: 0.2914 - val_accuracy: 0.8904\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 100s 1s/step - loss: 0.3044 - accuracy: 0.8741 - val_loss: 0.2550 - val_accuracy: 0.9254\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 99s 1s/step - loss: 0.2800 - accuracy: 0.8912 - val_loss: 0.2678 - val_accuracy: 0.9123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15687586248>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fitting model to images\n",
    "classifier.fit_generator(\n",
    "        train_set,\n",
    "        steps_per_epoch=98,\n",
    "        epochs=10,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prediction of single new data\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image= image.load_img(r\"C:\\Users\\Nanda Kumar\\Downloads\\chest-xray-images-for-classification-pneumonia\\pneumonia\\train\\NORMAL\\IM-0003-0001.jpeg\",target_size =(64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x156882C98C8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert image to array\n",
    "test_image = image.img_to_array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 34.,  34.,  34.],\n",
       "        [ 43.,  43.,  43.],\n",
       "        [ 89.,  89.,  89.],\n",
       "        ...,\n",
       "        [143., 143., 143.],\n",
       "        [126., 126., 126.],\n",
       "        [115., 115., 115.]],\n",
       "\n",
       "       [[ 34.,  34.,  34.],\n",
       "        [ 22.,  22.,  22.],\n",
       "        [ 85.,  85.,  85.],\n",
       "        ...,\n",
       "        [142., 142., 142.],\n",
       "        [122., 122., 122.],\n",
       "        [111., 111., 111.]],\n",
       "\n",
       "       [[ 38.,  38.,  38.],\n",
       "        [ 26.,  26.,  26.],\n",
       "        [ 77.,  77.,  77.],\n",
       "        ...,\n",
       "        [136., 136., 136.],\n",
       "        [123., 123., 123.],\n",
       "        [100., 100., 100.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 39.,  39.,  39.],\n",
       "        [ 39.,  39.,  39.],\n",
       "        [ 38.,  38.,  38.],\n",
       "        ...,\n",
       "        [ 33.,  33.,  33.],\n",
       "        [ 35.,  35.,  35.],\n",
       "        [ 37.,  37.,  37.]],\n",
       "\n",
       "       [[ 42.,  42.,  42.],\n",
       "        [ 39.,  39.,  39.],\n",
       "        [ 40.,  40.,  40.],\n",
       "        ...,\n",
       "        [ 34.,  34.,  34.],\n",
       "        [ 32.,  32.,  32.],\n",
       "        [ 33.,  33.,  33.]],\n",
       "\n",
       "       [[ 36.,  36.,  36.],\n",
       "        [ 42.,  42.,  42.],\n",
       "        [ 40.,  40.,  40.],\n",
       "        ...,\n",
       "        [ 32.,  32.,  32.],\n",
       "        [ 34.,  34.,  34.],\n",
       "        [ 32.,  32.,  32.]]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For single prediction change the dimension using axis. To remove problem of batch\n",
    "test_image = np.expand_dims(test_image,axis = 0)\n",
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class of normal and abnormal\n",
    "\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'normal'\n",
    "else:\n",
    "   prediction = 'abnormal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abnormal'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 34.,  34.,  34.],\n",
       "         [ 43.,  43.,  43.],\n",
       "         [ 89.,  89.,  89.],\n",
       "         ...,\n",
       "         [143., 143., 143.],\n",
       "         [126., 126., 126.],\n",
       "         [115., 115., 115.]],\n",
       "\n",
       "        [[ 34.,  34.,  34.],\n",
       "         [ 22.,  22.,  22.],\n",
       "         [ 85.,  85.,  85.],\n",
       "         ...,\n",
       "         [142., 142., 142.],\n",
       "         [122., 122., 122.],\n",
       "         [111., 111., 111.]],\n",
       "\n",
       "        [[ 38.,  38.,  38.],\n",
       "         [ 26.,  26.,  26.],\n",
       "         [ 77.,  77.,  77.],\n",
       "         ...,\n",
       "         [136., 136., 136.],\n",
       "         [123., 123., 123.],\n",
       "         [100., 100., 100.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 39.,  39.,  39.],\n",
       "         [ 39.,  39.,  39.],\n",
       "         [ 38.,  38.,  38.],\n",
       "         ...,\n",
       "         [ 33.,  33.,  33.],\n",
       "         [ 35.,  35.,  35.],\n",
       "         [ 37.,  37.,  37.]],\n",
       "\n",
       "        [[ 42.,  42.,  42.],\n",
       "         [ 39.,  39.,  39.],\n",
       "         [ 40.,  40.,  40.],\n",
       "         ...,\n",
       "         [ 34.,  34.,  34.],\n",
       "         [ 32.,  32.,  32.],\n",
       "         [ 33.,  33.,  33.]],\n",
       "\n",
       "        [[ 36.,  36.,  36.],\n",
       "         [ 42.,  42.,  42.],\n",
       "         [ 40.,  40.,  40.],\n",
       "         ...,\n",
       "         [ 32.,  32.,  32.],\n",
       "         [ 34.,  34.,  34.],\n",
       "         [ 32.,  32.,  32.]]]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
